{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hallpaz/drl/blob/main/notebooks/pratica_gymnasium.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O objetivo deste projeto é implementar um ambiente de simulação no framework Gym para resolver um problema específico. Neste problema, há uma coluna denominada \"Random\" contendo 50 posições, cada uma com um valor aleatório entre 1 e 100 (distribuição uniforme). Além disso, há um campo chamado \"Target\" que recebe um valor aleatório entre 1 e 100 (distribuição uniforme)."
      ],
      "metadata": {
        "id": "fBdOIgTkto0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A missão do agente é selecionar até 5 posições da coluna \"Random\", uma de cada vez, de modo que a soma dos valores dessas 5 posições alcance o valor do campo \"Target\", indicando o fim do episódio."
      ],
      "metadata": {
        "id": "FbOkD2Hl23iW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeb-XzPe6wZt",
        "outputId": "ae7a266a-ca9e-4d39-b74f-9bf8a6840f94"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.10.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "É importante destacar que o agente só pode \"olhar\" para uma posição de cada vez da coluna \"Random\".\n",
        "\n",
        "Um episódio é finalizado se a soma de target foi satisfeita ou todas os valores na coluna Random foram vistos ao menos 1 vez.\n",
        "\n",
        "Implementação:\n",
        "1 - Criação do Ambiente de simulação utilizando o framework Gym.\n",
        "2 - Definição inicial dos hiperparâmetros.\n",
        "3 - Arquitetura da Rede Neural com Tensorflow.\n",
        "4 - Treinamento do Agente.\n",
        "5 - Teste e Demonstração do Agente Maduro."
      ],
      "metadata": {
        "id": "Hec3FghOv4Q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "JfEJCy5hv6NP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COMBINE = 1\n",
        "DONT_COMBINE = 0"
      ],
      "metadata": {
        "id": "CfruZQklHiMm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MatchingEnv(gym.Env):\n",
        "  def __init__(self, options_size=10, target_size=1, options_limit=5):\n",
        "    self.options_size = options_size\n",
        "    self.target_size = target_size\n",
        "    self.options_limit = options_limit\n",
        "\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    self.options = np.random.randint(1, 101, self.options_size)\n",
        "    self.target = np.random.randint(1, 101, self.target_size)\n",
        "    self.current_option_index = 0\n",
        "    self.current_target_index = 0\n",
        "    self.selected = []\n",
        "\n",
        "    return (self.options[self.current_option_index],\n",
        "            self.target[self.current_target_index],\n",
        "            self.options_size - 1), {}\n",
        "\n",
        "  def step(self, action):\n",
        "    if action == COMBINE:\n",
        "      current_value = self.options[self.current_option_index]\n",
        "      self.selected.append(current_value)\n",
        "      # avança para o próximo valor\n",
        "      self.current_option_index += 1\n",
        "      # calcula o quanto falta em relação ao target\n",
        "      # self.target[self.current_target_index] -= current_value\n",
        "      remaining_value = self.target[self.current_target_index] - sum(self.selected)\n",
        "      # quantos passos ainda faltam\n",
        "      remaining_steps = self.options_size - self.current_option_index - 1\n",
        "\n",
        "      next_state = (self.options[self.current_option_index],\n",
        "                    remaining_value,\n",
        "                    remaining_steps)\n",
        "\n",
        "      # Falta recompensa\n",
        "    else:\n",
        "      # calcular proximo estado\n",
        "      # calcular recompensa\n",
        "      pass"
      ],
      "metadata": {
        "id": "7ofsj8rc7BPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A cada seleção que o agente aposta em fazer, as posições da coluna \"Random\" são sorteadas novamente, exceto aquelas que foram previamente escolhidas pelo agente."
      ],
      "metadata": {
        "id": "b4vYn6KrD-p_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MH3CqpCtllq"
      },
      "outputs": [],
      "source": [
        "# code"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follow up:\n",
        "* reduzir tamanho de array de números aleatórios.\n",
        "* gerar números dinamicamente em ciclos"
      ],
      "metadata": {
        "id": "462w0RuHN4aD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Folow up: Cobrar para que os alunos alterem o código para que o seguinte problema:\n",
        "Neste novo problema existirão duas colunas: \"Random1\" e \"Random2\".\n",
        "O agente poderá \"olhar\" para uma posição de cada coluna ao mesmo tempo.\n",
        "O agente deve encontrar o par de posições provenientes de \"Random1\" e \"Random2\" que multiplicados, sejam somados a um acumulador visando atingir o \"Target\". A cada par escolhido, \"Random1\" e \"Random2\" são atualizados."
      ],
      "metadata": {
        "id": "8_yZFxwmuTcx"
      }
    }
  ]
}