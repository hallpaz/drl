{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hallpaz/drl/blob/main/notebooks/proximal_policy_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBdOIgTkto0S"
      },
      "source": [
        "O objetivo deste projeto é implementar um ambiente de simulação no framework Gym para resolver um problema específico. Neste problema, há uma coluna denominada \"Random\" contendo 50 posições, cada uma com um valor aleatório entre 1 e 100 (distribuição uniforme). Além disso, há um campo chamado \"Target\" que recebe um valor aleatório entre 1 e 100 (distribuição uniforme)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbOkD2Hl23iW"
      },
      "source": [
        "A missão do agente é selecionar até 5 posições da coluna \"Random\", uma de cada vez, de modo que a soma dos valores dessas 5 posições alcance o valor do campo \"Target\", indicando o fim do episódio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeb-XzPe6wZt"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hec3FghOv4Q2"
      },
      "source": [
        "É importante destacar que o agente só pode \"olhar\" para uma posição de cada vez da coluna \"Random\".\n",
        "\n",
        "Um episódio é finalizado se a soma de target foi satisfeita ou todas os valores na coluna Random foram vistos ao menos 1 vez.\n",
        "\n",
        "Implementação:\n",
        "1 - Criação do Ambiente de simulação utilizando o framework Gym.\n",
        "2 - Definição inicial dos hiperparâmetros.\n",
        "3 - Arquitetura da Rede Neural com Tensorflow.\n",
        "4 - Treinamento do Agente.\n",
        "5 - Teste e Demonstração do Agente Maduro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfEJCy5hv6NP"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfruZQklHiMm"
      },
      "outputs": [],
      "source": [
        "COMBINE = 1\n",
        "DONT_COMBINE = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ofsj8rc7BPk"
      },
      "outputs": [],
      "source": [
        "class MatchingEnv(gym.Env):\n",
        "  def __init__(self, options_size=50, target_size=1,\n",
        "               options_limit=5, max_value=100):\n",
        "    self.options_size = options_size\n",
        "    self.target_size = target_size\n",
        "    self.options_limit = options_limit\n",
        "    self.max_value = max_value\n",
        "    self.action_space = [0, 1]\n",
        "\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    self.options = np.random.randint(1, self.max_value + 1, self.options_size)\n",
        "    self.target = np.random.randint(1, self.max_value + 1, self.target_size)\n",
        "    self.current_option_index = 0\n",
        "    self.current_target_index = 0\n",
        "    self.selected = []\n",
        "\n",
        "    return (self.options[self.current_option_index],\n",
        "            self.target[self.current_target_index],\n",
        "            self.options_size - 1,\n",
        "            len(self.selected))\n",
        "\n",
        "  def step(self, action):\n",
        "    done = False\n",
        "    reward = 0\n",
        "    if action == COMBINE:\n",
        "      current_value = self.options[self.current_option_index]\n",
        "      self.selected.append(current_value)\n",
        "      # avança para o próximo valor\n",
        "      self.current_option_index += 1\n",
        "      # calcula o quanto falta em relação ao target\n",
        "      # self.target[self.current_target_index] -= current_value\n",
        "      remaining_value = self.target[self.current_target_index] - sum(self.selected)\n",
        "      # quantos passos ainda faltam\n",
        "      remaining_steps = self.options_size - self.current_option_index - 1\n",
        "\n",
        "      if remaining_value > 0:\n",
        "        reward = current_value\n",
        "      elif remaining_value == 0:\n",
        "        reward = 100 * 10**(self.options_limit - len(self.selected))\n",
        "        done = True\n",
        "      else:\n",
        "        reward = -sum(self.selected)\n",
        "        done = True\n",
        "\n",
        "    else:\n",
        "      self.current_option_index += 1\n",
        "      # calcula o quanto falta em relação ao target\n",
        "      remaining_value = self.target[self.current_target_index] - sum(self.selected)\n",
        "      # quantos passos ainda faltam\n",
        "      remaining_steps = self.options_size - self.current_option_index - 1\n",
        "\n",
        "    if remaining_steps < 0 and not self.selected:\n",
        "      reward = -100\n",
        "\n",
        "    if remaining_steps < 0 or len(self.selected) >= self.options_limit or reward < 0:\n",
        "      done = True\n",
        "\n",
        "    next_state = (self.options[self.current_option_index % self.options_size],\n",
        "                remaining_value,\n",
        "                remaining_steps,\n",
        "                len(self.selected))\n",
        "    # print(next_state)\n",
        "    return next_state, reward, done, {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX47A_Ie5m6D"
      },
      "outputs": [],
      "source": [
        "env = MatchingEnv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4vYn6KrD-p_"
      },
      "source": [
        "A cada seleção que o agente aposta em fazer, as posições da coluna \"Random\" são sorteadas novamente, exceto aquelas que foram previamente escolhidas pelo agente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MH3CqpCtllq"
      },
      "outputs": [],
      "source": [
        "class Actor(tf.keras.Model):\n",
        "  def __init__(self, n_actions=2):\n",
        "    super().__init__()\n",
        "    self.d1 = tf.keras.layers.Dense(256, activation='relu')\n",
        "    self.d2 = tf.keras.layers.Dense(128, activation='relu')\n",
        "    self.a = tf.keras.layers.Dense(n_actions, activation='softmax')\n",
        "\n",
        "  def call(self, input_data):\n",
        "    x = self.d1(input_data)\n",
        "    x = self.d2(x)\n",
        "    a = self.a(x)\n",
        "    return a\n",
        "\n",
        "class Critic(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.d1 = tf.keras.layers.Dense(256, activation='relu')\n",
        "    self.d2 = tf.keras.layers.Dense(128, activation='relu')\n",
        "    self.v = tf.keras.layers.Dense(1, activation = None)\n",
        "\n",
        "  def call(self, input_data):\n",
        "    x = self.d1(input_data)\n",
        "    x = self.d2(x)\n",
        "    v = self.v(x)\n",
        "    return v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SNatRFW4Nqv"
      },
      "outputs": [],
      "source": [
        "# Source for the code: https://github.com/abhisheksuran/Reinforcement_Learning/blob/master/PPO.ipynb\n",
        "class Agent():\n",
        "    def __init__(self, gamma = 0.99, epsilon=0.2):\n",
        "        self.gamma = gamma\n",
        "        self.a_opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "        self.c_opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "        self.actor = Actor()\n",
        "        self.critic = Critic()\n",
        "        self.clip_pram = epsilon\n",
        "\n",
        "\n",
        "    def act(self,state):\n",
        "        prob = self.actor(np.array([state]))\n",
        "        prob = prob.numpy()\n",
        "        dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
        "        action = dist.sample()\n",
        "        return int(action.numpy()[0])\n",
        "\n",
        "\n",
        "    def actor_loss(self, probs, actions, adv, old_probs, closs):\n",
        "\n",
        "        probability = probs\n",
        "        entropy = tf.reduce_mean(tf.math.negative(tf.math.multiply(probability, tf.math.log(probability))))\n",
        "        sur1 = []\n",
        "        sur2 = []\n",
        "\n",
        "        for pb, t, op, a  in zip(probability, adv, old_probs, actions):\n",
        "                        t =  tf.constant(t)\n",
        "                        print(a)\n",
        "                        ratio = tf.math.divide(pb[a], op[a])\n",
        "\n",
        "                        s1 = tf.math.multiply(ratio, t)\n",
        "\n",
        "                        s2 =  tf.math.multiply(tf.clip_by_value(ratio, 1.0 - self.clip_pram, 1.0 + self.clip_pram), t)\n",
        "                        sur1.append(s1)\n",
        "                        sur2.append(s2)\n",
        "\n",
        "        sr1 = tf.stack(sur1)\n",
        "        sr2 = tf.stack(sur2)\n",
        "\n",
        "        loss = tf.math.negative(tf.reduce_mean(tf.math.minimum(sr1, sr2)) - closs + 0.001 * entropy)\n",
        "        return loss\n",
        "\n",
        "    def learn(self, states, actions,  adv , old_probs, discnt_rewards):\n",
        "        discnt_rewards = tf.reshape(discnt_rewards, (len(discnt_rewards),))\n",
        "        adv = tf.reshape(adv, (len(adv),))\n",
        "\n",
        "        old_p = tf.reshape(old_probs, (len(old_probs), 2))\n",
        "        # with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            p = self.actor(states, training=True)\n",
        "            v =  self.critic(states,training=True)\n",
        "            v = tf.reshape(v, (len(v),))\n",
        "            td = tf.math.subtract(discnt_rewards, v)\n",
        "            c_loss = 0.5 * tf.keras.losses.mean_squared_error(discnt_rewards, v)\n",
        "            a_loss = self.actor_loss(p, actions, adv, old_probs, c_loss)\n",
        "\n",
        "        grads1 = tape.gradient(a_loss, self.actor.trainable_variables)\n",
        "        grads2 = tape.gradient(c_loss, self.critic.trainable_variables)\n",
        "        self.a_opt.apply_gradients(zip(grads1, self.actor.trainable_variables))\n",
        "        self.c_opt.apply_gradients(zip(grads2, self.critic.trainable_variables))\n",
        "        return a_loss, c_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_reward(agent, env):\n",
        "  total_reward = 0\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  while not done:\n",
        "    action = np.argmax(agent.actor(np.array([state])).numpy())\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    state = next_state\n",
        "    total_reward += reward\n",
        "\n",
        "  return total_reward"
      ],
      "metadata": {
        "id": "BaZt7VMdlwv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXPOYtHuOG1Y"
      },
      "outputs": [],
      "source": [
        "agent = Agent()\n",
        "steps = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kraOGGtx4PQU"
      },
      "outputs": [],
      "source": [
        "def preprocess(states, actions, rewards, done, values, gamma):\n",
        "    # generalized advantage estimation\n",
        "    g = 0\n",
        "    lmbda = 0.95\n",
        "    returns = []\n",
        "    for i in reversed(range(len(rewards))):\n",
        "       delta = rewards[i] + gamma * values[i + 1] * done[i] - values[i]\n",
        "       g = delta + gamma * lmbda * dones[i] * g\n",
        "       returns.append(g + values[i])\n",
        "\n",
        "    returns.reverse()\n",
        "    adv = np.array(returns, dtype=np.float32) - values[:-1]\n",
        "    adv = (adv - np.mean(adv)) / (np.std(adv) + 1e-10)\n",
        "    states = np.array(states, dtype=np.float32)\n",
        "    actions = np.array(actions, dtype=np.int32)\n",
        "    returns = np.array(returns, dtype=np.float32)\n",
        "    return states, actions, returns, adv\n",
        "\n",
        "\n",
        "tf.random.set_seed(7777777)\n",
        "gamma = 1\n",
        "ep_reward = []\n",
        "total_avgr = []\n",
        "target = False\n",
        "best_reward = 0\n",
        "avg_rewards_list = []\n",
        "\n",
        "\n",
        "for s in range(steps):\n",
        "  done = False\n",
        "  state = env.reset()\n",
        "  all_aloss = []\n",
        "  all_closs = []\n",
        "  rewards = []\n",
        "  states = []\n",
        "  actions = []\n",
        "  probs = []\n",
        "  dones = []\n",
        "  values = []\n",
        "  print(s, \"new episod\")\n",
        "\n",
        "  for e in range(128):\n",
        "\n",
        "    action = agent.act(state)\n",
        "    value = agent.critic(np.array([state])).numpy()\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    dones.append(1-done)\n",
        "    rewards.append(reward)\n",
        "    states.append(state)\n",
        "    actions.append(action)\n",
        "    prob = agent.actor(np.array([state]))\n",
        "    probs.append(prob[0])\n",
        "    values.append(value[0][0])\n",
        "    state = next_state\n",
        "    if done:\n",
        "      env.reset()\n",
        "\n",
        "  value = agent.critic(np.array([state])).numpy()\n",
        "  values.append(value[0][0])\n",
        "  np.reshape(probs, (len(probs),2))\n",
        "  probs = np.stack(probs, axis=0)\n",
        "\n",
        "  states, actions, returns, adv  = preprocess(states, actions, rewards, dones, values, gamma)\n",
        "\n",
        "  for epoch in range(10):\n",
        "      al, cl = agent.learn(states, actions, adv, probs, returns)\n",
        "\n",
        "  avg_reward = np.mean([test_reward(agent, env) for _ in range(5)])\n",
        "  print(f\"total test reward is {avg_reward}\")\n",
        "  avg_rewards_list.append(avg_reward)\n",
        "  if avg_reward > best_reward:\n",
        "        print('best reward=' + str(avg_reward))\n",
        "        agent.actor.save('model_actor_{}_{}'.format(s, avg_reward), save_format=\"tf\")\n",
        "        agent.critic.save('model_critic_{}_{}'.format(s, avg_reward), save_format=\"tf\")\n",
        "        best_reward = avg_reward\n",
        "\n",
        "  env.reset()\n",
        "\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}